---
title: "Session 2: GDPUSAvar (non Seasonal) ARMA Modelling"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Aim: Identify (and fit) at least 2 plausible non-Seasonal ARMA(p,q) models for the GDPUSAvar time series

### GDPUSAvar: gross domestic product variation of the United States. Quarter I-1990 - Quarter IV-2019

Percent change over the previous period (quarter) of the gross domestic product of the United States; official web http://www.bea.gov/national/index.htm. Quarter I-1990 - Quarter IV-2019; 30 years. The series is seasonally adjusted.



### Introductory Remarks:

This Lab-session is organized in three parts:

- Part I. Transforming time series into stationarity: Concepts covered in the first (theory and lab) session.

- Part II: Non-seasonal ARMA model identification. Focused on ARMA models identification.

- Partt III: Model estimation and validation. Outputs of estimation and validation.

Comments/Recommendations:

- Revise the Part I of this lab session and be sure to fully understand it (based on already covered lab session S1).

- Take a look of the Part II after you have studied the theory presented in the corresponding slides/video.

- Part III will be partially covered in today’s class; any remaining explanations will be given throughout the next classes devoted to Box & Jenking’s models.

### I. Exploratory data analysis and transformation into stationarity

#### Ia. Exploratory data analysis

Import the data

```{r}
(serie=ts((read.table("gdpUSAvar.dat",header=F)),start=1990,freq=4))
```

Plot the time series GDPUSAvar

```{r}
plot(serie)
abline(v=1990:2020,col=4,lty=3)
abline(h=0)

# We can see that there are trends in some intervals but overall it seems that the mean will be constant
```

#### Remarks…

- Mention/describe here all known events in the period of study that might have influenced the AirBcn time series: Gulf war, Twin towers fall, financial crisis,…

- Provide your general statistical impressions. For instance, does it seems to be:

  - a seasonal component?

  - a global trend? A non-constant superposed to a non-constant mean behavior.

  - a non constant variance? Let’s check it all…

#### Ib. Tranforming series into a stationary one:

Is Variance constant?

```{r}
boxplot(serie~floor(time(serie)))

# It is not informative because we take 4 observations for each boxplot, which is non-informative of the variance and the median, so we need to group observatins (at least 10 or at the very least 8)
```
Boxplots with only 4 observations? Better group observations of 2-3 years (8-12 quarters)?

```{r}
groupedserie <- matrix(serie, ncol=12) #12 quarters=3yrs together=ncol=12
boxplot(groupedserie)

# The variance does not seem to be very different from lower values to larger
```

Thus, … the varianza is considered constant!

#### Is seasonality present?

Nop, GDPUSAvar series has been previously seasonally adjusted and it represents already percentage variation

#### Is the mean constant?

Plot again the whole time series together with its mean value

```{r}
plot(serie)
abline(h=0)
abline(h=mean(serie),col=2)

```
```{r}
mean(serie) # This. would be the slope of the trend of the original series with seasonality
```

Mean seemingly constant, but different from zero!!!

#### Check for over-differentiation: Take an extra differentiation and compare the variances.

```{r}
d1serie=diff(serie)
plot(d1serie)
abline(h=0)
abline(h=mean(d1serie),col=2)
```

```{r}
var(serie)
var(d1serie)
```

#### Is an extra differentiation needed?

NOPE! Observe that with extra regular difference the mean becomes zero, but the variance increases…

#### Is the current series already stationary? Plot the ACF and decide!!

```{r}
par(mar = c(5, 4, 4, 2))
acf(serie,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,3)), main ="ACF(serie)")

```


Notice that the ACF decays fast towards zero and thus the dependence structure depends only on the lag. Thus, the GDPUSAvar time series is already stationary!

In this case, the given series GDPUSAvar shows no seasonality, has constant mean and variance. Additionally, its covariance structure does not depends on the time-origin.

### II. Model Identification

Proceed to identify possible values of p and q of the ARMA(p,q) model by inspecting both, the ACF and PACF.

#### Selection of plausible models to fit: propose >=2 models (principle of parsimony!)

```{r}
par(mfrow=c(1,2), mar = c(5, 4, 4, 2))
acf(serie,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,3)), main ="ACF(serie)",lwd=2)
pacf(serie,ylim=c(-1,1),lag.max=60,col=c(rep(1,3),2), main ="PACF(serie)",lwd=2)
par(mfrow=c(1,1))
```


Three models proposed for the stationary GDPUSAvar (serie) time series: AR(2), MA(2), ARMA(1,1)

### III. Estimation and validation

#### ‘Load’ the function ‘validation’: used later


```{r}
#################Validation#################################
validation=function(model,dades){
  s=frequency(get(model$series))
  resid=model$residuals
  par(mfrow=c(2,2),mar=c(3,3,3,3))
  #Residuals plot
  plot(resid,main="Residuals")
  abline(h=0)
  abline(h=c(-3*sd(resid),3*sd(resid)),lty=3,col=4)
  #Square Root of absolute values of residuals (Homocedasticity)
  scatter.smooth(sqrt(abs(resid)),main="Square Root of Absolute residuals",
                 lpars=list(col=2))
  
  #Normal plot of residuals
  qqnorm(resid)
  qqline(resid,col=2,lwd=2)
  
  ##Histogram of residuals with normal curve
  hist(resid,breaks=20,freq=F)
  curve(dnorm(x,mean=mean(resid),sd=sd(resid)),col=2,add=T)
  
  
  #ACF & PACF of residuals
  par(mfrow=c(1,2))
  acf(resid,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
  pacf(resid,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
  par(mfrow=c(1,1))
  
  #ACF & PACF of square residuals 
  par(mfrow=c(1,2))
  acf(resid^2,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
  pacf(resid^2,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
  par(mfrow=c(1,1))
    
  #Ljung-Box p-values
  par(mar=c(2,2,1,1))
  tsdiag(model,gof.lag=7*s)
  cat("\n--------------------------------------------------------------------\n")
  print(model)
  
  #Stationary and Invertible
  cat("\nModul of AR Characteristic polynomial Roots: ", 
      Mod(polyroot(c(1,-model$model$phi))),"\n")
  cat("\nModul of MA Characteristic polynomial Roots: ",
      Mod(polyroot(c(1,model$model$theta))),"\n")
  
  #Model expressed as an MA infinity (psi-weights)
  psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
  names(psis)=paste("psi",1:36)
  cat("\nPsi-weights (MA(inf))\n")
  cat("\n--------------------\n")
  print(psis[1:20])
  
  #Model expressed as an AR infinity (pi-weights)
  pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
  names(pis)=paste("pi",1:36)
  cat("\nPi-weights (AR(inf))\n")
  cat("\n--------------------\n")
  print(pis[1:20])
  
  ##Shapiro-Wilks Normality test
  print(shapiro.test(resid(model)))
  
  #Sample ACF vs. Teoric ACF
  par(mfrow=c(2,2),mar=c(3,3,3,3))
  acf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample ACF")
  
  plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36),ylim=c(-1,1), 
       type="h",xlab="Lag",  ylab="", main="ACF Teoric")
  abline(h=0)
  
  #Sample PACF vs. Teoric PACF
  pacf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample PACF")
  
  plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36, pacf=T),ylim=c(-1,1),
       type="h", xlab="Lag", ylab="", main="PACF Teoric")
  abline(h=0)
  par(mfrow=c(1,1))
}
################# Fi Validaci? #################################
```

### IIIa. Estimation and validation of model 1

#### Model 1: AR(2) process

```{r}
(model1=arima(serie,order=c(2,0,0)))
```

#### Check significance of estimated parameters

Remember: ML estimators are asymptotically normally distributed. Use this result to check significance of the 3 estimated parameters.

```{r}
cat("Z values:",fill=TRUE)
abs(model1$coef/sqrt(diag(model1$var.coef)))
```

All significant since |coeff/SE_coeff| > 2 (or either contruct and check 95%CI)

Thus the expression of the AR(2) fitted model is:
$$(1-\phi_1B-\phi_2B^2)(W_t-\mu_{W_t})=Z_t \qquad Z_t\sim N(0,\sigma^2_Z)$$
$$(1-0.3048B-0.1942B^2)(W_t-2.4989)=Z_t \qquad Z_t\sim N(0,\sigma^2_Z=4.484)$$
$$W_t=(1-0.3048-0.1942)*2.4989+0.3048W_{t-1}+0.1942W_{t-2}+Z_t \qquad Z_t\sim N(0,\sigma^2_Z=4.484)$$

$$W_t=1.2519+0.3048W_{t-1}+0.1942W_{t-2}+Z_t \qquad Z_t\sim N(0,\sigma^2_Z=4.484)$$

#### Validation of Model 1:

AR(2) process validation: several diagnostic tools used

```{r}
dades=serie
model=model1
validation(model,dades)
```


#### Final conclusion: Is your proposed model reasonably valid? Justify your answer!

The asumptions are reasonably fulfilled, but be aware of possible outliers (we will study them later on!)

**Reminder**: Residuals must be WN (white noise), and thus must not cointain information (Normal, centered at zero with constant variance, and independent).

**To do**: Be sure you know how to explain all validation outputs!

### IIIb. Estimation and validation of model 2

#### Model 2: MA(2) process

```{r}
(model2=arima(serie,order=c(0,0,2)))
```

#### Validation of Model 2:

```{r}
dades=serie
model=model2
validation(model,dades)
```

### IIIc. Estimation and validation of model 3

#### Model 3: ARMA(1,1) process

```{r}
(model3=arima(serie,order=c(1,0,1)))
```

```{r}
dades=serie
model=model3
validation(model,dades)
```

### Summary of Adequacy Measures Criteria

```{r}
resul=data.frame(
  Sigma2Z=c(model1$sigma2, model2$sigma2, model3$sigma2),
  AIC=c(AIC(model1), AIC(model2), AIC(model3)),
  BIC=c(BIC(model1), BIC(model2), BIC(model3)))
row.names(resul)=c("ARMA(2,0)", "ARMA(0,2)", "ARMA(1,1)")


resul
```


#### **To do**: Select a model among the three fitted ones. Justify your answer!

Reminder: make sure to write down the **statistical-mathematical expression of your ARMA(p,q) model** (in terms of the characteristic polynomials).
