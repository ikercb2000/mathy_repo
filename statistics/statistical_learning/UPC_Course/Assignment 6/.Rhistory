knitr::opts_chunk$set(echo = TRUE)
library(splines)
library(tidyr)
load("bikes.Washington.Rdata")
load("bikes.Rdata")
load("bikes.Washington.Rdata")
load("bikes.Washington.Rdata")
#cnt <- scale(bikes$cnt, center = TRUE, scale = FALSE)
#instant <- scale(bikes$instant)
cnt=bikes$cnt
instant = bikes$instant
cubic.reg.spline <- smooth.spline(x = instant, y = cnt, cv = FALSE)
tibble(data.frame(lambda = cubic.reg.spline$lambda,
edf = round(cubic.reg.spline$df,2),
n.knots = cubic.reg.spline$n)) %>%
knitr::kable(.)
plot(instant, cnt,pch=16,cex=0.5,col="red")
lines(cubic.reg.spline, col = "blue", lwd = 3)
k <- 3; x <- instant; y <- cnt #inputs
n.knots <- cubic.reg.spline$df - 4 #instructions section e
my.knots <- quantile(instant,((1:n.knots)-.5)/n.knots) #instructions section e
# Create a basis
basis <- bs(x=x,knots=my.knots,intercept=T,degree=k)
# Unpenalized regression using the basis
lm.spl <- lm(y~basis)
# Plotting results
plot(x,y,col=2,xlab="instant",ylab="cnt",pch=16,cex=0.5)
lines(x,lm.spl$fitted.values, col = "green", lwd = 2)
abline(v=my.knots,lty=1,col="grey")
lines(cubic.reg.spline, col = "blue", lwd = 2)
legend("topleft", legend = c("Cubic Regression Spline",
"Unpenalized Cubic Regression Splines"),
col = c("blue", "green"), lty = c(1,1), lwd = 2)
bikes$cnt.5000 <- ifelse(bikes$cnt>=5000,1,0)
# logistic.IRWLS.splines function
logistic.IRWLS.splines <- function(x,y,weights.out=1,x.new=x,
df=6,spar=NULL,
all.knots = FALSE, nknots = .nknots.smspl,
max.iter=10,eps.beta=1e-5,
plts=FALSE){
if (plts&(dim(as.matrix(x))[2]>1)){
plts<-FALSE
warning("Plots are valid only when dim(x)[2]==1")
}
stop.rule <- FALSE
iter <- 0
theta.0 <- fitted(lm(y~x))
while (!stop.rule){
iter <- iter + 1
p <- p.from.theta(theta.0)
ps.e <- (y-p)/(p*(1-p))
z <- theta.0 + ps.e
wt <- p*(1-p) *weights.out
if (plts){
op<-par(mfrow=c(1,2))
plot(x,y,cex=8*wt)
lines(x,p,col=2)
plot(x,z,cex=8*wt)
lines(x,theta.0,col=2)
par(op)
}
spline.1 <- smooth.spline(x,z,w=wt,df=df,spar=spar,
all.knots = all.knots,
nknots = nknots)
theta.1 <- predict(spline.1,x=x)$y
if ((iter>=max.iter)|(sum((theta.1-theta.0)^2)<eps.beta)){
stop.rule<-TRUE
} else {
theta.0 <- theta.1
}
}
p <- p.from.theta(theta.1)
resid.devi <- sum(-2*dbinom(y,1,p,log=TRUE))
return(list(fitted.values=p,
theta.x=theta.1,
df=spline.1$df,
predicted.values=p.from.theta(predict(spline.1,x=x.new)$y),
residual.deviance=resid.devi)
)
}
# p.from.theta function
p.from.theta <- function(theta.x){
p.x <- 1/(1+exp(-theta.x))
return(p.x)
}
# Non-parametric logistic regression estimation through IRWLS
log_spline <- logistic.IRWLS.splines(bikes$temp,bikes$cnt.5000,plts=TRUE)
# Range obtention
fit.log <- log_spline$fitted.values
bikes$fit.log <- fit.log
temp_05 <- bikes$temp[bikes$fit.log>0.5]
paste0("The range of temp for which P(cnt>=5000|temp)>0.5 goes from ",round(min(temp_05),2)," to ",round(max(temp_05),2))
loglik_KCV_df = function(X, Y, k, dfs, seed){
set.seed(seed)
# Splitting the data into k groups
indexes = matrix(0,nrow=dim(X)[1]/k,ncol=k) #Each column is one sub-sample
numbers = 1:dim(X)[1]
for (i in 1:k){
ind = sample(numbers,dim(X)[1]/k,replace = FALSE)
indexes[,i] = ind
numbers = setdiff(numbers,ind)
}
# Creating a matrix of loglikelhoods
loglik_matrix <- matrix(0, nrow=length(dfs), ncol=k)
#Each column is the loglikelihoods for each df associated to one of the k samples
for (i in 1:k){
training_X = X[indexes[,-i],]
test_X = X[indexes[,-i],]
training_Y = Y[indexes[,-i]]
test_Y = Y[indexes[,-i]]
loglik_matrix[,i]=logit(training_X, test_X, training_Y, test_Y, dfs)
}
# Computing the mean loglikelihood associated to each df
dfs_loglik = apply(loglik_matrix, MARGIN = 1, FUN = mean)
return(list("values" = dfs_loglik,"optimal df" = df.v[which.max(dfs_loglik)]))
}
logit<-function(training_X,test_X,training_Y,test_Y,dfs){
loglikelihoods <- numeric(length(dfs))
for (i in 1:length(dfs)) {
mod_log <- logistic.IRWLS.splines(y=training_Y,
x=training_X, df=dfs[i],
plts=FALSE, x.new=test_X)
pred <- mod_log$predicted.values
res <- mean(test_Y*log(pred/(1-pred))+log(1-pred))
loglikelihoods[i] <- res
}
return(loglikelihoods)
}
# k-fold cross validation function
df.v <- 3:15
cnt.5000 <- bikes[,10]
temp <- bikes[,6]
# loglikCVdf(x = temp, y = cnt.5000, k = 5, dfs = df.v)
loglik_KCV_df(X = as.data.frame(temp), Y = cnt.5000, k = 5, dfs = df.v, seed = 123)
logistic.IRWLS.splines <- function(x,y,weights.out=1,x.new=x,
df=6,spar=NULL,
all.knots = FALSE, nknots = .nknots.smspl,
max.iter=10,eps.beta=1e-5,
plts=FALSE){
if (plts&(dim(as.matrix(x))[2]>1)){
plts<-FALSE
warning("Plots are valid only when dim(x)[2]==1")
}
stop.rule <- FALSE
iter <- 0
theta.0 <- fitted(lm(y~x))
while (!stop.rule){
iter <- iter + 1
p <- p.from.theta(theta.0)
ps.e <- (y-p)/(p*(1-p))
z <- theta.0 + ps.e
wt <- p*(1-p) *weights.out
if (plts){
op<-par(mfrow=c(1,2))
plot(x,y,cex=8*wt)
lines(x,p,col=2)
plot(x,z,cex=8*wt)
lines(x,theta.0,col=2)
par(op)
}
spline.1 <- smooth.spline(x,z,w=wt,df=df,spar=spar,
all.knots = all.knots,
nknots = nknots)
theta.1 <- predict(spline.1,x=x)$y
if ((iter>=max.iter)|(sum((theta.1-theta.0)^2)<eps.beta)){
stop.rule<-TRUE
} else {
theta.0 <- theta.1
}
}
p <- p.from.theta(theta.1)
resid.devi <- sum(-2*dbinom(y,1,p,log=TRUE))
return(list(fitted.values=p,
theta.x=theta.1,
df=spline.1$df,
predicted.values=p.from.theta(predict(spline.1,x=x.new)$y),
residual.deviance=resid.devi)
)
}
# p.from.theta function
p.from.theta <- function(theta.x){
p.x <- 1/(1+exp(-theta.x))
return(p.x)
}
df.v <- 3:30
cnt.5000 <- bikes[,10]
temp <- bikes[,6]
# loglikCVdf(x = temp, y = cnt.5000, k = 5, dfs = df.v)
loglik_KCV_df(X = as.data.frame(temp), Y = cnt.5000, k = 5, dfs = df.v, seed = 123)
